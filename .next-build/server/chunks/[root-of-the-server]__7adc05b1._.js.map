{"version":3,"sources":["../../../lib/openai.js","../../../lib/documentAI.js"],"sourcesContent":["// OpenAI API integration with official SDK\r\nimport OpenAI from 'openai';\r\n\r\nconst openai = new OpenAI({\r\n  apiKey: process.env.OPENAI_API_KEY,\r\n});\r\n\r\n// Generate embeddings for text\r\nexport async function generateEmbedding(text) {\r\n  if (!process.env.OPENAI_API_KEY) {\r\n    throw new Error('OPENAI_API_KEY is not configured. Please set it in your environment variables.');\r\n  }\r\n\r\n  try {\r\n    const response = await openai.embeddings.create({\r\n      model: 'text-embedding-3-small',\r\n      input: text.substring(0, 8000), // Limit input size\r\n    });\r\n\r\n    return response.data[0].embedding;\r\n  } catch (error) {\r\n    console.error('Error generating embedding:', error);\r\n    throw new Error(`Failed to generate embedding: ${error.message}`);\r\n  }\r\n}\r\n\r\n// Generate embeddings in batch for an array of texts\r\nexport async function generateEmbeddingsBatch(texts) {\r\n  if (!process.env.OPENAI_API_KEY) {\r\n    throw new Error('OPENAI_API_KEY is not configured. Please set it in your environment variables.');\r\n  }\r\n\r\n  if (!Array.isArray(texts) || texts.length === 0) return [];\r\n\r\n  try {\r\n    const response = await openai.embeddings.create({\r\n      model: 'text-embedding-3-small',\r\n      input: texts.map(t => (t || '').toString().substring(0, 8000)),\r\n    });\r\n\r\n    return response.data.map(d => d.embedding);\r\n  } catch (error) {\r\n    console.error('Error generating batch embeddings:', error);\r\n    throw new Error(`Failed to generate batch embeddings: ${error.message}`);\r\n  }\r\n}\r\n\r\n// Chat completion using OpenAI GPT models\r\nexport async function chatCompletion(messages, options = {}) {\r\n  if (!process.env.OPENAI_API_KEY) {\r\n    throw new Error('OPENAI_API_KEY is not configured. Please set it in your environment variables.');\r\n  }\r\n\r\n  try {\r\n    const response = await openai.chat.completions.create({\r\n      model: options.model || 'gpt-4o-mini', // Fast and affordable\r\n      messages: messages,\r\n      temperature: options.temperature || 0.7,\r\n      max_tokens: options.max_tokens || 1000,\r\n      top_p: options.top_p || 1,\r\n      frequency_penalty: options.frequency_penalty || 0,\r\n      presence_penalty: options.presence_penalty || 0,\r\n    });\r\n\r\n    return response.choices[0].message.content;\r\n  } catch (error) {\r\n    console.error('Error in chat completion:', error);\r\n    \r\n    // Handle specific OpenAI errors\r\n    if (error.status === 401) {\r\n      throw new Error('Chiave API OpenAI non valida. Verifica la configurazione.');\r\n    } else if (error.status === 429) {\r\n      throw new Error('Limite di rate raggiunto. Riprova tra qualche secondo.');\r\n    } else if (error.status === 500) {\r\n      throw new Error('Errore del server OpenAI. Riprova più tardi.');\r\n    }\r\n    \r\n    throw new Error(`Errore nella comunicazione con OpenAI: ${error.message}`);\r\n  }\r\n}\r\n\r\n// Generate summary\r\nexport async function generateSummary(text, maxLength = 200) {\r\n  const messages = [\r\n    {\r\n      role: 'system',\r\n      content: 'You are a helpful assistant that creates concise summaries.',\r\n    },\r\n    {\r\n      role: 'user',\r\n      content: `Summarize the following text in ${maxLength} words or less:\\n\\n${text}`,\r\n    },\r\n  ];\r\n\r\n  return await chatCompletion(messages, { temperature: 0.5, max_tokens: 300 });\r\n}\r\n\r\n// Generate tags\r\nexport async function generateTags(text, count = 5) {\r\n  const messages = [\r\n    {\r\n      role: 'system',\r\n      content: 'You are a helpful assistant that generates relevant tags for documents. Return only comma-separated tags, no explanations.',\r\n    },\r\n    {\r\n      role: 'user',\r\n      content: `Generate ${count} relevant tags for this document:\\n\\n${text.substring(0, 2000)}`,\r\n    },\r\n  ];\r\n\r\n  const response = await chatCompletion(messages, { temperature: 0.3, max_tokens: 100 });\r\n  \r\n  return response\r\n    .split(',')\r\n    .map(tag => tag.trim().toLowerCase())\r\n    .filter(tag => tag.length > 0)\r\n    .slice(0, count);\r\n}\r\n\r\n// Classify document\r\nexport async function classifyDocument(text) {\r\n  const categories = [\r\n    'contract', 'invoice', 'report', 'presentation', \r\n    'spreadsheet', 'image', 'code', 'email', 'article', 'other'\r\n  ];\r\n\r\n  const messages = [\r\n    {\r\n      role: 'system',\r\n      content: `You are a document classifier. Classify the document into one or more of these categories: ${categories.join(', ')}. Return only category names, comma-separated.`,\r\n    },\r\n    {\r\n      role: 'user',\r\n      content: `Classify this document:\\n\\n${text.substring(0, 1000)}`,\r\n    },\r\n  ];\r\n\r\n  const response = await chatCompletion(messages, { temperature: 0.2, max_tokens: 50 });\r\n  \r\n  return response\r\n    .split(',')\r\n    .map(cat => cat.trim().toLowerCase())\r\n    .filter(cat => categories.includes(cat));\r\n}\r\n\r\n// Vision API - Analizza immagini come ChatGPT\r\nexport async function analyzeImageWithVision(imageBuffer, imageMimeType, query = 'Cosa contiene questa immagine? Descrivi tutto ciò che vedi.') {\r\n  if (!process.env.OPENAI_API_KEY) {\r\n    throw new Error('OPENAI_API_KEY is not configured. Please set it in your environment variables.');\r\n  }\r\n\r\n  try {\r\n    // Converti il buffer in base64\r\n    const base64Image = imageBuffer.toString('base64');\r\n    \r\n    const response = await openai.chat.completions.create({\r\n      model: 'gpt-4o', // GPT-4o supporta vision\r\n      messages: [\r\n        {\r\n          role: 'system',\r\n          content: 'Sei un assistente AI esperto nell\\'analisi di immagini. Descrivi in dettaglio tutto ciò che vedi nell\\'immagine, inclusi testo, oggetti, persone, scene, colori e qualsiasi altro dettaglio rilevante. Rispondi sempre in italiano.'\r\n        },\r\n        {\r\n          role: 'user',\r\n          content: [\r\n            {\r\n              type: 'text',\r\n              text: query\r\n            },\r\n            {\r\n              type: 'image_url',\r\n              image_url: {\r\n                url: `data:${imageMimeType};base64,${base64Image}`,\r\n                detail: 'high'\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      ],\r\n      max_tokens: 2000,\r\n      temperature: 0.2\r\n    });\r\n\r\n    return response.choices[0].message.content;\r\n  } catch (error) {\r\n    console.error('Error in vision API:', error);\r\n    \r\n    if (error.status === 401) {\r\n      throw new Error('Chiave API OpenAI non valida. Verifica la configurazione.');\r\n    } else if (error.status === 429) {\r\n      throw new Error('Limite di rate raggiunto. Riprova tra qualche secondo.');\r\n    } else if (error.status === 400 && error.message?.includes('image')) {\r\n      throw new Error('Formato immagine non supportato o immagine troppo grande.');\r\n    }\r\n    \r\n    throw new Error(`Errore nell'analisi dell'immagine: ${error.message}`);\r\n  }\r\n}\r\n\r\n// Estrazione testo da PDF con OpenAI Responses API (gpt-4o) usando input_file\r\n// Ritorna SOLO la stringa di testo estratto (no IDs, no oggetti)\r\nexport async function extractTextFromPdfWithOpenAI(filePath) {\r\n  if (!process.env.OPENAI_API_KEY) {\r\n    throw new Error('OPENAI_API_KEY is not configured. Please set it in your environment variables.');\r\n  }\r\n\r\n  const fs = await import('fs');\r\n  \r\n  let uploadedFile = null;\r\n  try {\r\n    console.log('Caricamento PDF su OpenAI (Responses API):', filePath);\r\n    \r\n    // 1) Carica il file su OpenAI come user_data\r\n    uploadedFile = await openai.files.create({\r\n      file: fs.createReadStream(filePath),\r\n      purpose: 'user_data'\r\n    });\r\n\r\n    console.log('File caricato su OpenAI:', uploadedFile.id);\r\n\r\n    // 2) Richiesta al modello GPT-4o per estrazione testo dal file caricato\r\n    const response = await openai.responses.create({\r\n      model: 'gpt-4o',\r\n      input: [\r\n        {\r\n          role: 'user',\r\n          content: [\r\n            { type: 'input_text', text: 'Estrai tutto il testo e le informazioni rilevanti da questo documento. Rispondi SOLO con il testo estratto, senza commenti.' },\r\n            { type: 'input_file', file_id: uploadedFile.id }\r\n          ]\r\n        }\r\n      ],\r\n      max_output_tokens: 8000,\r\n      temperature: 0.1\r\n    });\r\n\r\n    // 3) Prendi solo testo\r\n    const extractedText = (response.output_text || '').toString().trim();\r\n    console.log(`Testo estratto da PDF: ${extractedText.length} caratteri`);\r\n    \r\n    if (!extractedText) {\r\n      throw new Error('Nessun testo estratto dal PDF');\r\n    }\r\n\r\n    return extractedText;\r\n\r\n  } catch (error) {\r\n    console.error('Error in PDF text extraction (Responses API):', error);\r\n    \r\n    if (error.status === 401) {\r\n      throw new Error('Chiave API OpenAI non valida. Verifica la configurazione.');\r\n    } else if (error.status === 429) {\r\n      throw new Error('Limite di rate raggiunto. Riprova tra qualche secondo.');\r\n    } else if (error.status === 400) {\r\n      throw new Error('Errore nell\\'analisi del PDF con Responses API. Il file potrebbe essere troppo grande o danneggiato.');\r\n    }\r\n    \r\n    throw new Error(`Errore nell'estrazione del testo PDF: ${error.message}`);\r\n  } finally {\r\n    // 4) Prova a rimuovere il file caricato (se l'SDK lo supporta)\r\n    try {\r\n      if (uploadedFile && uploadedFile.id) {\r\n        if (openai.files?.delete) {\r\n          await openai.files.delete(uploadedFile.id);\r\n        } else if (openai.files?.del) {\r\n          await openai.files.del(uploadedFile.id);\r\n        }\r\n        console.log('File rimosso da OpenAI:', uploadedFile.id);\r\n      }\r\n    } catch (delError) {\r\n      console.warn('Errore rimozione file da OpenAI (ignorato):', delError?.message || delError);\r\n    }\r\n  }\r\n}","// Sistema AI completo per analisi documenti con OpenAI GPT-4\r\nimport mammoth from 'mammoth';\r\nimport * as XLSX from 'xlsx';\r\nimport { chatCompletion } from './openai.js';\r\n\r\n// pdf-parse ha problemi ESM/CJS con Turbopack, usiamo solo pdfjs-dist\r\n\r\n// Storage in-memory per i documenti caricati (in produzione si può usare database)\r\nconst documentStore = new Map(); // { fileId: { text, chunks, metadata } }\r\n\r\n/**\r\n * Estrae testo da un documento\r\n * @param {Buffer} buffer - Buffer del file\r\n * @param {string} mimeType - Tipo MIME del file\r\n * @param {string} filename - Nome del file\r\n * @param {string} [filepath] - Percorso opzionale del file (utile per OCR)\r\n */\r\nexport async function extractTextFromDocument(buffer, mimeType, filename, filepath = null) {\r\n  try {\r\n    let text = '';\r\n    let metadata = {\r\n      filename,\r\n      mimeType,\r\n      pages: 0,\r\n      wordCount: 0,\r\n    };\r\n\r\n    if (mimeType === 'application/pdf' || filename.toLowerCase().endsWith('.pdf')) {\r\n      // PDF: usa OpenAI file upload + GPT extraction (tutto lato server OpenAI)\r\n      const { extractTextFromPdfWithOpenAI } = await import('./openai.js');\r\n      console.log(`Analizzando PDF con OpenAI file upload: ${filename}`);\r\n      \r\n      // Salva temporaneamente il file per l'upload\r\n      const fs = await import('fs');\r\n      const path = await import('path');\r\n      const os = await import('os');\r\n      \r\n      const tempDir = os.tmpdir();\r\n      const tempFilePath = path.join(tempDir, `temp_${Date.now()}_${filename}`);\r\n      \r\n      try {\r\n        // Scrivi il buffer su file temporaneo\r\n        fs.writeFileSync(tempFilePath, buffer);\r\n        \r\n        // Estrai testo con OpenAI (file upload -> GPT risponde con SOLO testo)\r\n        text = await extractTextFromPdfWithOpenAI(tempFilePath);\r\n        \r\n        // Assicurati che sia una stringa pulita\r\n        text = (text || \"\").toString().trim();\r\n        metadata.analysisMethod = 'openai-file-upload';\r\n        \r\n        if (!text || text.length === 0) {\r\n          throw new Error('Nessun testo estratto dal PDF tramite OpenAI');\r\n        }\r\n        \r\n        console.log(`PDF analizzato con OpenAI: ${text.length} caratteri estratti`);\r\n      } finally {\r\n        // Pulisci file temporaneo\r\n        try {\r\n          if (fs.existsSync(tempFilePath)) {\r\n            fs.unlinkSync(tempFilePath);\r\n          }\r\n        } catch (cleanupError) {\r\n          console.warn('Errore pulizia file temporaneo:', cleanupError);\r\n        }\r\n      }\r\n    }\r\n    else if (mimeType.includes('wordprocessingml') || filename.toLowerCase().endsWith('.docx')) {\r\n      const result = await mammoth.extractRawText({ buffer });\r\n      text = result.value;\r\n      \r\n      // Pulisci il testo\r\n      text = text.replace(/\\s+/g, ' ').trim();\r\n    }\r\n    else if (mimeType.includes('spreadsheet') || filename.toLowerCase().match(/\\.(xlsx|xls|csv)$/i)) {\r\n      const workbook = XLSX.read(buffer, { type: 'buffer' });\r\n      const sheets = workbook.SheetNames;\r\n      \r\n      // Estrai testo da tutte le celle\r\n      const allText = [];\r\n      for (const sheetName of sheets) {\r\n        const sheet = workbook.Sheets[sheetName];\r\n        const sheetData = XLSX.utils.sheet_to_json(sheet, { header: 1, defval: '' });\r\n        \r\n        for (const row of sheetData) {\r\n          if (Array.isArray(row)) {\r\n            const rowText = row.filter(cell => cell && cell.toString().trim()).join(' ');\r\n            if (rowText.trim()) {\r\n              allText.push(`[${sheetName}] ${rowText}`);\r\n            }\r\n          }\r\n        }\r\n      }\r\n      \r\n      text = allText.join('\\n');\r\n    }\r\n    else if (mimeType.startsWith('text/') || filename.toLowerCase().endsWith('.txt')) {\r\n      text = buffer.toString('utf-8');\r\n    }\r\n    else if (mimeType.startsWith('image/') || filename.toLowerCase().match(/\\.(png|jpg|jpeg|gif|webp|bmp|tif|tiff|heic|heif)$/i)) {\r\n      // Usa OpenAI Vision API come ChatGPT (più intelligente) + OCR come fallback\r\n      try {\r\n        const { analyzeImageWithVision } = await import('./openai.js');\r\n        console.log(`Analizzando immagine con OpenAI Vision API: ${filename}`);\r\n        \r\n        try {\r\n          // Prova prima con OpenAI Vision API (come ChatGPT)\r\n          const visionText = await analyzeImageWithVision(\r\n            buffer, \r\n            mimeType,\r\n            'Analizza questa immagine in dettaglio. Descrivi tutto ciò che vedi, incluso qualsiasi testo presente, oggetti, persone, scene, colori e dettagli rilevanti. Se c\\'è del testo, trascrivilo accuratamente.'\r\n          );\r\n          \r\n          text = visionText;\r\n          console.log(`Vision API completato: ${text.length} caratteri estratti`);\r\n          \r\n          // Aggiungi metadata per indicare che è stato usato Vision API\r\n          metadata.analysisMethod = 'openai-vision';\r\n        } catch (visionError) {\r\n          console.log('Vision API fallito, provo con OCR:', visionError.message);\r\n          \r\n          // Fallback a OCR tradizionale\r\n          const Tesseract = (await import('tesseract.js')).default;\r\n          const imageSource = filepath || buffer;\r\n          \r\n          try {\r\n            const result = await Tesseract.recognize(imageSource, 'ita+eng', {\r\n              logger: (m) => {\r\n                if (m.status === 'recognizing text') {\r\n                  console.log(`OCR Progress: ${Math.round(m.progress * 100)}%`);\r\n                }\r\n              }\r\n            });\r\n            \r\n            text = result.data.text.trim();\r\n            metadata.analysisMethod = 'ocr-tesseract';\r\n          } catch (ocrError) {\r\n            // Se anche OCR fallisce, prova con createWorker\r\n            console.log('OCR semplice fallito, provo con createWorker:', ocrError.message);\r\n            \r\n            const worker = await Tesseract.createWorker('ita+eng', 1, {\r\n              logger: (m) => {\r\n                if (m.status === 'recognizing text') {\r\n                  console.log(`OCR Progress: ${Math.round(m.progress * 100)}%`);\r\n                }\r\n              }\r\n            });\r\n            \r\n            try {\r\n              const result = await worker.recognize(imageSource);\r\n              text = result.data.text.trim();\r\n              metadata.analysisMethod = 'ocr-tesseract-worker';\r\n            } finally {\r\n              await worker.terminate();\r\n            }\r\n          }\r\n        }\r\n        \r\n        if (!text || text.length === 0) {\r\n          throw new Error('Nessun contenuto estratto dall\\'immagine. L\\'immagine potrebbe essere vuota o non contenere testo leggibile.');\r\n        }\r\n        \r\n        console.log(`Analisi immagine completata: ${text.length} caratteri estratti (metodo: ${metadata.analysisMethod || 'unknown'})`);\r\n      } catch (imageError) {\r\n        console.error('Errore analisi immagine completo:', imageError);\r\n        throw new Error(`Errore nell'analisi dell'immagine: ${imageError.message}`);\r\n      }\r\n    }\r\n    else {\r\n      throw new Error(`Tipo di file non supportato: ${mimeType}`);\r\n    }\r\n\r\n    if (!text || text.trim().length === 0) {\r\n      throw new Error('Nessun testo estratto dal documento');\r\n    }\r\n\r\n    metadata.wordCount = text.split(/\\s+/).length;\r\n\r\n    return { text, metadata };\r\n  } catch (error) {\r\n    console.error('Errore estrazione testo:', error);\r\n    throw new Error(`Errore nell'estrazione del testo: ${error.message}`);\r\n  }\r\n}\r\n\r\n/**\r\n * Divide il testo in chunk intelligenti per la ricerca semantica\r\n * Genera anche embeddings per ogni chunk (ricerca semantica vera)\r\n */\r\nexport async function createTextChunks(text, chunkSize = 1000, overlap = 200) {\r\n  const sentences = text.split(/[.!?]+\\s+/).filter(s => s.trim().length > 10);\r\n  const chunks = [];\r\n  \r\n  let currentChunk = '';\r\n  let currentLength = 0;\r\n\r\n  for (const sentence of sentences) {\r\n    const sentenceLength = sentence.length;\r\n    \r\n    if (currentLength + sentenceLength > chunkSize && currentChunk.trim()) {\r\n      // Salva il chunk corrente\r\n      chunks.push({\r\n        text: currentChunk.trim(),\r\n        startIndex: text.indexOf(currentChunk.trim()),\r\n        endIndex: text.indexOf(currentChunk.trim()) + currentChunk.length,\r\n      });\r\n\r\n      // Crea nuovo chunk con overlap (ultime parole del chunk precedente)\r\n      const words = currentChunk.split(/\\s+/);\r\n      const overlapWords = words.slice(-Math.floor(overlap / 10));\r\n      currentChunk = overlapWords.join(' ') + ' ' + sentence + ' ';\r\n      currentLength = currentChunk.length;\r\n    } else {\r\n      currentChunk += sentence + '. ';\r\n      currentLength += sentenceLength + 2;\r\n    }\r\n  }\r\n\r\n  // Aggiungi l'ultimo chunk\r\n  if (currentChunk.trim()) {\r\n    chunks.push({\r\n      text: currentChunk.trim(),\r\n      startIndex: text.indexOf(currentChunk.trim()),\r\n      endIndex: text.length,\r\n    });\r\n  }\r\n\r\n  const finalChunks = chunks.length > 0 ? chunks : [{ text: text.trim(), startIndex: 0, endIndex: text.length }];\r\n  \r\n  // Genera embeddings per i chunk in batch per ridurre latenza\r\n  const { generateEmbeddingsBatch } = await import('./openai.js');\r\n\r\n  try {\r\n    // Processa in lotti per evitare payload troppo grandi\r\n    const BATCH_SIZE = 16;\r\n    for (let i = 0; i < finalChunks.length; i += BATCH_SIZE) {\r\n      const batch = finalChunks.slice(i, i + BATCH_SIZE);\r\n      const embeddings = await generateEmbeddingsBatch(batch.map(c => c.text));\r\n      embeddings.forEach((emb, idx) => {\r\n        batch[idx].embedding = emb;\r\n      });\r\n    }\r\n  } catch (embError) {\r\n    console.warn('Batch embeddings failed, falling back to per-chunk:', embError.message);\r\n    const { generateEmbedding } = await import('./openai.js');\r\n    for (const chunk of finalChunks) {\r\n      try {\r\n        chunk.embedding = await generateEmbedding(chunk.text);\r\n      } catch (err) {\r\n        console.warn('Errore generazione embedding per chunk:', err.message);\r\n        chunk.embedding = null;\r\n      }\r\n    }\r\n  }\r\n  \r\n  return finalChunks;\r\n}\r\n\r\n/**\r\n * Calcola cosine similarity tra due vettori di embedding\r\n */\r\nfunction cosineSimilarity(vecA, vecB) {\r\n  if (!vecA || !vecB || vecA.length !== vecB.length) return 0;\r\n  \r\n  const dot = vecA.reduce((sum, val, i) => sum + val * vecB[i], 0);\r\n  const magA = Math.sqrt(vecA.reduce((sum, val) => sum + val * val, 0));\r\n  const magB = Math.sqrt(vecB.reduce((sum, val) => sum + val * val, 0));\r\n  \r\n  return dot / (magA * magB);\r\n}\r\n\r\n/**\r\n * Calcola TF-IDF per ricerca semantica semplice (fallback se embedding non disponibile)\r\n */\r\nfunction calculateTFIDF(text, query) {\r\n  const textWords = text.toLowerCase().split(/\\W+/).filter(w => w.length > 2);\r\n  const queryWords = query.toLowerCase().split(/\\W+/).filter(w => w.length > 2);\r\n  \r\n  if (queryWords.length === 0) return 0;\r\n\r\n  let score = 0;\r\n  const textWordCount = {};\r\n  textWords.forEach(word => {\r\n    textWordCount[word] = (textWordCount[word] || 0) + 1;\r\n  });\r\n\r\n  queryWords.forEach(queryWord => {\r\n    const wordCount = textWordCount[queryWord] || 0;\r\n    if (wordCount > 0) {\r\n      // TF: frequenza del termine nel documento\r\n      const tf = wordCount / textWords.length;\r\n      // IDF: inversa della frequenza (più raro = più importante)\r\n      const idf = Math.log(textWords.length / (wordCount + 1)) + 1;\r\n      score += tf * idf;\r\n    }\r\n  });\r\n\r\n  return score / queryWords.length;\r\n}\r\n\r\n/**\r\n * Ricerca semantica nel documento usando embeddings (cosine similarity)\r\n * Fallback a TF-IDF se embeddings non disponibili\r\n */\r\nexport async function semanticSearch(documentText, chunks, query, topK = 5) {\r\n  if (!chunks || chunks.length === 0) {\r\n    return [];\r\n  }\r\n\r\n  // Genera embedding per la query\r\n  const { generateEmbedding } = await import('./openai.js');\r\n  let queryEmbedding = null;\r\n  \r\n  try {\r\n    queryEmbedding = await generateEmbedding(query);\r\n  } catch (embError) {\r\n    console.warn('Errore generazione embedding query, uso TF-IDF:', embError);\r\n  }\r\n\r\n  // Calcola score per ogni chunk\r\n  const scoredChunks = chunks.map((chunk, index) => {\r\n    let semanticScore = 0;\r\n    \r\n    // Usa cosine similarity se embeddings disponibili\r\n    if (queryEmbedding && chunk.embedding) {\r\n      semanticScore = cosineSimilarity(queryEmbedding, chunk.embedding);\r\n    } else {\r\n      // Fallback a TF-IDF\r\n      semanticScore = calculateTFIDF(chunk.text, query) / 10; // Normalizza per essere simile a cosine\r\n    }\r\n    \r\n    // Bonus per match esatti\r\n    const lowerChunk = chunk.text.toLowerCase();\r\n    const lowerQuery = query.toLowerCase();\r\n    let exactMatchBonus = 0;\r\n    if (lowerChunk.includes(lowerQuery)) {\r\n      exactMatchBonus = 0.1;\r\n    }\r\n    \r\n    // Bonus per parole chiave comuni\r\n    const queryWords = query.toLowerCase().split(/\\W+/).filter(w => w.length > 2);\r\n    const matchingWords = queryWords.filter(word => lowerChunk.includes(word)).length;\r\n    const keywordBonus = (matchingWords / queryWords.length) * 0.05;\r\n\r\n    const totalScore = semanticScore + exactMatchBonus + keywordBonus;\r\n\r\n    return {\r\n      ...chunk,\r\n      score: totalScore,\r\n      index,\r\n    };\r\n  });\r\n\r\n  // Ordina per score e ritorna i top K (anche con score bassi)\r\n  const sortedChunks = scoredChunks\r\n    .sort((a, b) => b.score - a.score)\r\n    .slice(0, topK);\r\n  \r\n  console.log('Top chunk scores:', sortedChunks.map(c => c.score.toFixed(4)));\r\n  \r\n  // Ritorna anche risultati con score bassi (meglio di niente)\r\n  return sortedChunks.filter(chunk => chunk.score >= 0);\r\n}\r\n\r\n/**\r\n * Genera risposta intelligente basata sul contenuto del documento usando OpenAI GPT-4\r\n * @param {string} query - Domanda dell'utente\r\n * @param {Array} relevantChunks - Chunk rilevanti dal documento\r\n * @param {string} documentText - Testo completo del documento\r\n * @param {string} conversationContext - Contesto della conversazione (opzionale)\r\n */\r\nexport async function generateAnswer(query, relevantChunks, documentText, conversationContext = '', documentMetadata = {}) {\r\n  if (!relevantChunks || relevantChunks.length === 0) {\r\n    return {\r\n      answer: \"Non ho trovato informazioni rilevanti nel documento per rispondere alla tua domanda. Prova a formulare la domanda in modo diverso o carica un documento più pertinente.\",\r\n      confidence: 0,\r\n      sources: [],\r\n    };\r\n  }\r\n\r\n  // Combina i chunk più rilevanti - SOLO TESTO (no oggetti, no IDs)\r\n  const combinedContext = relevantChunks\r\n    .map((chunk, idx) => `[Sezione ${idx + 1}]\\n${chunk.text}`)\r\n    .join('\\n\\n---\\n\\n');\r\n\r\n  // Usa OpenAI GPT-4 per generare una risposta intelligente\r\n  const confidence = Math.min(relevantChunks[0].score * 10, 1.0);\r\n  \r\n  try {\r\n    // Costruisci il prompt con contesto della conversazione se disponibile\r\n    let contextSection = '';\r\n    if (conversationContext && conversationContext.trim()) {\r\n      contextSection = `\\n\\n**CONTESTO DELLA CONVERSAZIONE PRECEDENTE:**\r\n${conversationContext}\r\n\r\nUsa questo contesto per capire meglio la domanda e fornire una risposta coerente con la conversazione.`;\r\n    }\r\n\r\n    const messages = [\r\n      {\r\n        role: 'system',\r\n        content: `Sei un assistente AI esperto nell'analisi di documenti. Segui un ragionamento interno SILENTE (non mostrarlo) con i passi: 1) Identifica concetti chiave 2) Seleziona parti rilevanti 3) Sintetizza relazioni 4) Verifica contraddizioni o assenze 5) Struttura risposta finale. Poi produci SOLO l'output formattato.\r\n\r\nOBIETTIVI:\r\n1. Analisi accurata delle sezioni fornite\r\n2. Risposta logica e coerente basata esclusivamente sul contenuto\r\n3. Struttura chiara e professionale in italiano\r\n4. Citazione delle fonti/Sezioni rilevanti\r\n5. Nessuna invenzione: se manca l'informazione, dichiaralo e suggerisci cosa servirebbe\r\n\r\nFORMATTA LA RISPOSTA CON LE SEZIONI (usa markdown semplice):\r\n**Risposta**: sintesi principale diretta alla domanda.\r\n**Dettagli**: approfondisci punti chiave organizzati.\r\n**Fonti**: elenco puntato con sezione e breve estratto.\r\n**Limiti**: cosa non è presente o ambiguo.\r\n**Suggerimenti**: eventuali passi successivi o chiarimenti da richiedere.\r\n\r\nRegole:\r\n- Non includere il ragionamento interno.\r\n- Non scusarti a meno che il contenuto sia realmente assente.\r\n- Mantieni tono professionale, conciso ma completo.`\r\n      },\r\n      {\r\n        role: 'user',\r\n        content: `Analizza il seguente contenuto estratto dal documento e rispondi alla domanda dell'utente in modo completo e accurato.\r\n\r\n**CONTENUTO DEL DOCUMENTO:**\r\n${combinedContext}${contextSection}\r\n\r\n**DOMANDA DELL'UTENTE:**\r\n${query}\r\n\r\n**ISTRUZIONI:**\r\n- Rispondi basandoti SOLO sul contenuto fornito sopra\r\n- Se la risposta richiede informazioni non presenti, dillo chiaramente\r\n- Cita o fai riferimento alle sezioni rilevanti quando possibile\r\n- Fornisci una risposta completa e ben strutturata\r\n- Considera il contesto della conversazione se fornito per una risposta più pertinente`\r\n      }\r\n    ];\r\n    \r\n    let answer = await chatCompletion(messages, {\r\n      model: 'gpt-4o-mini', // Modello veloce e economico\r\n      temperature: 0.2, // Bassa temperatura per risposte più accurate e consistenti\r\n      max_tokens: 1200, // Aumentato per risposte più complete\r\n      top_p: 0.9,\r\n      frequency_penalty: 0.1, // Riduce ripetizioni\r\n      presence_penalty: 0.1 // Incentiva varietà\r\n    });\r\n    answer = answer.trim();\r\n    // Post-processing: se il modello ha prodotto solo una frase debole/apologetica, arricchisci con estratti\r\n    const isWeak = /mi dispiace|non posso/i.test(answer) && answer.length < 120;\r\n    if (isWeak) {\r\n      const enriched = `**Risposta**: Informazioni limitate nel testo fornito.\r\n**Dettagli**:\\n${relevantChunks.slice(0,3).map(c=>'- '+c.text.substring(0,200).replace(/\\n+/g,' ')+ (c.text.length>200?'...':'' )).join('\\n')}\\n**Fonti**:\\n${relevantChunks.map((c,i)=>`- Sezione ${i+1} (score ${c.score.toFixed(3)})`).join('\\n')}\\n**Limiti**: Il documento sembra contenere testo estremamente breve o generico.\\n**Suggerimenti**: Carica un documento più esteso oppure specifica meglio la domanda.`;\r\n      answer = enriched;\r\n    }\r\n    return {\r\n      answer,\r\n      confidence,\r\n      sources: relevantChunks.map(c => ({\r\n        filename: documentMetadata?.originalFilename || documentMetadata?.filename || 'Documento',\r\n        text: c.text.substring(0, 150) + '...',\r\n        score: c.score.toFixed(3),\r\n      })),\r\n    };\r\n  } catch (error) {\r\n    console.error('Errore nella generazione della risposta con OpenAI:', error);\r\n    // Fallback a risposta semplice se OpenAI fallisce\r\n    const contextText = relevantChunks[0].text;\r\n    return {\r\n      answer: `Basandomi sul documento:\\n\\n${contextText}\\n\\n${relevantChunks.length > 1 ? '\\nInformazioni aggiuntive:\\n' + relevantChunks.slice(1, 3).map((c, i) => `• ${c.text.substring(0, 200)}...`).join('\\n') : ''}`,\r\n      confidence,\r\n      sources: relevantChunks.map(c => ({\r\n        filename: documentMetadata?.originalFilename || documentMetadata?.filename || 'Documento',\r\n        text: c.text.substring(0, 150) + '...',\r\n        score: c.score.toFixed(3),\r\n      })),\r\n    };\r\n  }\r\n}\r\n\r\n/**\r\n * Salva documento nello storage (con embeddings per ricerca semantica)\r\n */\r\nexport async function storeDocument(fileId, text, metadata = {}) {\r\n  console.log(`storeDocument called for ${fileId}, text length: ${text.length}`);\r\n  \r\n  const chunks = await createTextChunks(text);\r\n  \r\n  console.log(`Created ${chunks.length} chunks for document ${fileId}`);\r\n  \r\n  const documentData = {\r\n    text,\r\n    chunks,\r\n    metadata: {\r\n      ...metadata,\r\n      filename: metadata.filename || metadata.originalFilename || 'Unknown',\r\n      storedAt: new Date().toISOString(),\r\n      chunkCount: chunks.length,\r\n    },\r\n  };\r\n  \r\n  documentStore.set(fileId, documentData);\r\n  \r\n  console.log(`Document ${fileId} stored. Store size now: ${documentStore.size}`);\r\n  console.log(`Store keys:`, Array.from(documentStore.keys()));\r\n\r\n  return {\r\n    fileId,\r\n    chunkCount: chunks.length,\r\n    wordCount: text.split(/\\s+/).length,\r\n  };\r\n}\r\n\r\n/**\r\n * Recupera documento dallo storage\r\n */\r\nexport function getDocument(fileId) {\r\n  return documentStore.get(fileId);\r\n}\r\n\r\n/**\r\n * Rimuovi documento dallo storage\r\n */\r\nexport function removeDocument(fileId) {\r\n  return documentStore.delete(fileId);\r\n}\r\n\r\n/**\r\n * Cerca in tutti i documenti (async per embeddings)\r\n */\r\nexport async function searchAllDocuments(query, fileIds = null) {\r\n  console.log('searchAllDocuments called with query:', query, 'fileIds:', fileIds);\r\n  console.log('documentStore size:', documentStore.size);\r\n  console.log('documentStore keys:', Array.from(documentStore.keys()));\r\n  \r\n  const documents = fileIds && fileIds.length > 0\r\n    ? fileIds.map(id => {\r\n        const data = documentStore.get(id);\r\n        console.log(`Looking for fileId ${id}:`, data ? 'found' : 'NOT FOUND');\r\n        return { id, data };\r\n      }).filter(d => d.data)\r\n    : Array.from(documentStore.entries()).map(([id, data]) => {\r\n        console.log(`Document ${id}:`, data ? 'found' : 'NOT FOUND');\r\n        return { id, data };\r\n      });\r\n\r\n  console.log(`Found ${documents.length} documents to search`);\r\n\r\n  if (documents.length === 0) {\r\n    console.log('No documents found to search in');\r\n    return [];\r\n  }\r\n\r\n  const allResults = [];\r\n\r\n  for (const { id, data } of documents) {\r\n    if (!data || !data.chunks || !data.chunks.length) {\r\n      console.log(`Document ${id} has no chunks, skipping`);\r\n      continue;\r\n    }\r\n\r\n    console.log(`Searching in document ${id} (${data.chunks.length} chunks)`);\r\n    const results = await semanticSearch(data.text, data.chunks, query, 5);\r\n    \r\n    if (results.length > 0) {\r\n      console.log(`Found ${results.length} relevant chunks in document ${id}`);\r\n      allResults.push({\r\n        fileId: id,\r\n        filename: data.metadata?.originalFilename || data.metadata?.filename || 'Unknown',\r\n        results,\r\n        topScore: results[0].score,\r\n      });\r\n    } else {\r\n      console.log(`No relevant chunks found in document ${id}`);\r\n    }\r\n  }\r\n\r\n  console.log(`Total search results: ${allResults.length}`);\r\n\r\n  // Ordina per score migliore\r\n  return allResults.sort((a, b) => b.topScore - a.topScore);\r\n}\r\n\r\n/**\r\n * Genera risposta combinando risultati da più documenti usando OpenAI\r\n * @param {string} query - Domanda dell'utente\r\n * @param {Array} searchResults - Risultati della ricerca nei documenti\r\n * @param {string} conversationContext - Contesto della conversazione (opzionale)\r\n */\r\nexport async function generateMultiDocumentAnswer(query, searchResults, conversationContext = '') {\r\n  if (!searchResults || searchResults.length === 0) {\r\n    return {\r\n      answer: \"Non ho trovato informazioni rilevanti nei documenti caricati. Prova a formulare la domanda in modo diverso o carica documenti più pertinenti.\",\r\n      confidence: 0,\r\n      sources: [],\r\n    };\r\n  }\r\n\r\n  // Prendi i chunk migliori da tutti i documenti\r\n  const topChunks = searchResults\r\n    .flatMap(fileResult => \r\n      fileResult.results.map(r => ({\r\n        ...r,\r\n        filename: fileResult.filename,\r\n        fileId: fileResult.fileId,\r\n      }))\r\n    )\r\n    .sort((a, b) => b.score - a.score)\r\n    .slice(0, 5);\r\n\r\n  if (topChunks.length === 0) {\r\n    return {\r\n      answer: \"Non ho trovato informazioni sufficienti per rispondere.\",\r\n      confidence: 0,\r\n      sources: [],\r\n    };\r\n  }\r\n\r\n  // Genera risposta combinata usando OpenAI\r\n  const primaryChunk = topChunks[0];\r\n  const document = getDocument(primaryChunk.fileId);\r\n  \r\n  if (searchResults.length === 1) {\r\n    // Un solo documento - usa generateAnswer con metadata per filename\r\n    return await generateAnswer(query, topChunks, document.text, conversationContext, document.metadata || {});\r\n  } else {\r\n    // Multiple documenti - combina informazioni da più fonti\r\n    const confidence = Math.min(primaryChunk.score * 10, 1.0);\r\n    \r\n    try {\r\n      // Combina SOLO testo dai documenti (no file IDs, no oggetti)\r\n      const combinedContext = topChunks\r\n        .map((chunk, idx) => `[Documento: \"${chunk.filename}\" - Sezione ${idx + 1}]\\n${chunk.text}`)\r\n        .join('\\n\\n---\\n\\n');\r\n      \r\n      // Costruisci il prompt con contesto della conversazione se disponibile\r\n      let contextSection = '';\r\n      if (conversationContext && conversationContext.trim()) {\r\n        contextSection = `\\n\\n**CONTESTO DELLA CONVERSAZIONE PRECEDENTE:**\r\n${conversationContext}\r\n\r\nUsa questo contesto per capire meglio la domanda e fornire una risposta coerente con la conversazione.`;\r\n      }\r\n\r\n      const messages = [\r\n        {\r\n          role: 'system',\r\n          content: `Sei un assistente AI esperto nell'analisi di documenti multipli. Usa ragionamento interno SILENTE (non mostrarlo) seguendo: 1) Mappa concetti per documento 2) Trova sovrapposizioni/contrasti 3) Sintetizza 4) Valuta completezza 5) Struttura output. Non mostrare i passi.\r\n\r\nFORMATTA OUTPUT:\r\n**Risposta** (sintesi diretta alla domanda)\r\n**Analisi** (integrazione logica tra documenti, relazioni, eventuali differenze)\r\n**Fonti** (elenco: Documento – breve estratto pertinente)\r\n**Limiti** (informazioni mancanti, contraddizioni non risolte)\r\n**Suggerimenti** (cosa potrebbe aiutare o prossimi passi)\r\n\r\nRegole:\r\n- Non inventare contenuto\r\n- Non scusarti salvo assenza totale di informazioni\r\n- Cita il nome del documento per ogni informazione specifica\r\n- Indica contraddizioni se presenti.`\r\n        },\r\n        {\r\n          role: 'user',\r\n          content: `Analizza il contenuto estratto da ${searchResults.length} documenti diversi e rispondi alla domanda dell'utente sintetizzando le informazioni rilevanti.\r\n\r\n**CONTENUTO DAI DOCUMENTI:**\r\n${combinedContext}${contextSection}\r\n\r\n**DOMANDA DELL'UTENTE:**\r\n${query}\r\n\r\n**ISTRUZIONI:**\r\n- Combina informazioni da tutti i documenti rilevanti\r\n- Cita sempre il nome del documento quando fai riferimento a informazioni specifiche\r\n- Se ci sono informazioni correlate in più documenti, integrale in modo coerente\r\n- Fornisci una risposta completa che risponda alla domanda utilizzando tutte le fonti pertinenti\r\n- Considera il contesto della conversazione se fornito per una risposta più pertinente`\r\n        }\r\n      ];\r\n      \r\n      let answer = await chatCompletion(messages, {\r\n        model: 'gpt-4o-mini',\r\n        temperature: 0.2,\r\n        max_tokens: 1500, // Aumentato per risposte multi-documento più complete\r\n        top_p: 0.9,\r\n        frequency_penalty: 0.1,\r\n        presence_penalty: 0.1\r\n      });\r\n      answer = answer.trim();\r\n      const isWeak = /mi dispiace|non posso/i.test(answer) && answer.length < 140;\r\n      if (isWeak) {\r\n        const enrichment = topChunks.slice(0,5).map((c,i)=>`- ${c.filename} [score ${c.score.toFixed(3)}]: ${c.text.substring(0,160).replace(/\\n+/g,' ')}${c.text.length>160?'...':''}`).join('\\n');\r\n        answer = `**Risposta**: Informazioni limitate ma estratte dai documenti.\r\n**Analisi**: I contenuti disponibili sono molto brevi; non emergono argomentazioni complesse.\r\n**Fonti**:\\n${enrichment}\r\n**Limiti**: Poca profondità; serve testo aggiuntivo.\r\n**Suggerimenti**: Carica versioni più complete dei documenti o specifica una domanda più dettagliata.`;\r\n      }\r\n      return {\r\n        answer,\r\n        confidence,\r\n        sources: topChunks.slice(0, 3).map(c => ({\r\n          filename: c.filename,\r\n          text: c.text.substring(0, 100) + '...',\r\n          score: c.score.toFixed(3),\r\n        })),\r\n      };\r\n    } catch (error) {\r\n      console.error('Errore nella generazione della risposta multi-documento con OpenAI:', error);\r\n      // Fallback\r\n      let answer = `Basandomi sui ${searchResults.length} documenti caricati:\\n\\n`;\r\n      answer += `**Dal documento \"${primaryChunk.filename}\":**\\n${primaryChunk.text}\\n\\n`;\r\n      \r\n      if (topChunks.length > 1) {\r\n        const otherDocs = topChunks.slice(1, 3).filter(c => c.fileId !== primaryChunk.fileId);\r\n        if (otherDocs.length > 0) {\r\n          answer += `**Informazioni correlate da altri documenti:**\\n`;\r\n          otherDocs.forEach((chunk, idx) => {\r\n            answer += `\\n[${idx + 1}] Da \"${chunk.filename}\":\\n${chunk.text.substring(0, 200)}...\\n`;\r\n          });\r\n        }\r\n      }\r\n      \r\n      const confidence = Math.min(primaryChunk.score * 10, 1.0);\r\n      \r\n      return {\r\n        answer: answer.trim(),\r\n        confidence,\r\n        sources: topChunks.slice(0, 3).map(c => ({\r\n          filename: c.filename,\r\n          text: c.text.substring(0, 100) + '...',\r\n          score: c.score.toFixed(3),\r\n        })),\r\n      };\r\n    }\r\n  }\r\n}\r\n\r\n/**\r\n * Ottieni tutti i documenti con i loro ID\r\n */\r\nexport function getAllDocuments() {\r\n  return Array.from(documentStore.entries()).map(([fileId, doc]) => ({\r\n    fileId,\r\n    ...doc\r\n  }));\r\n}\r\n\r\n/**\r\n * Ottieni statistiche sui documenti caricati\r\n */\r\nexport function getDocumentStats() {\r\n  const documents = Array.from(documentStore.values());\r\n  \r\n  return {\r\n    totalDocuments: documentStore.size,\r\n    totalWords: documents.reduce((sum, doc) => sum + (doc.text.split(/\\s+/).length || 0), 0),\r\n    totalChunks: documents.reduce((sum, doc) => sum + (doc.chunks?.length || 0), 0),\r\n    documents: Array.from(documentStore.entries()).map(([fileId, doc]) => ({\r\n      fileId,\r\n      index: Array.from(documentStore.keys()).indexOf(fileId),\r\n      filename: doc.metadata?.filename || doc.metadata?.originalFilename || 'Unknown',\r\n      wordCount: doc.text.split(/\\s+/).length,\r\n      chunkCount: doc.chunks?.length || 0,\r\n      storedAt: doc.metadata?.storedAt,\r\n    })),\r\n  };\r\n}\r\n\r\n"],"names":[],"mappings":"qIACA,IAAA,EAAA,EAAA,CAAA,CAAA,yCAEA,IAAM,EAAS,IAAI,EAAA,OAAM,CAAC,CACxB,OAAQ,QAAQ,GAAG,CAAC,cAAc,AACpC,GAGO,eAAe,EAAkB,CAAI,EAC1C,GAAI,CAAC,QAAQ,GAAG,CAAC,cAAc,CAC7B,CAD+B,KACzB,AAAI,MAAM,kFAGlB,GAAI,CAMF,MAAO,CALU,MAAM,EAAO,UAAU,CAAC,MAAM,CAAC,CAC9C,MAAO,yBACP,MAAO,EAAK,SAAS,CAAC,EAAG,IAC3B,EAAA,EAEgB,IAAI,CAAC,EAAE,CAAC,SAAS,AACnC,CAAE,MAAO,EAAO,CAEd,MADA,QAAQ,KAAK,CAAC,8BAA+B,GACnC,AAAJ,MAAU,CAAC,8BAA8B,EAAE,EAAM,OAAO,CAAA,CAAE,CAClE,CACF,CAGO,eAAe,EAAwB,CAAK,EACjD,GAAI,CAAC,QAAQ,GAAG,CAAC,cAAc,CAC7B,CAD+B,KACzB,AAAI,MAAM,kFAGlB,GAAI,CAAC,MAAM,OAAO,CAAC,IAA2B,IAAjB,EAAM,MAAM,CAAQ,MAAO,EAAE,CAE1D,GAAI,CAMF,MAAO,CALU,MAAM,EAAO,UAAU,CAAC,MAAM,CAAC,CAC9C,MAAO,yBACP,MAAO,EAAM,GAAG,CAAC,GAAK,AAAC,IAAK,EAAA,CAAE,CAAE,QAAQ,GAAG,SAAS,CAAC,EAAG,KAC1D,EAAA,EAEgB,IAAI,CAAC,GAAG,CAAC,GAAK,EAAE,SAAS,CAC3C,CAAE,MAAO,EAAO,CAEd,MADA,QAAQ,KAAK,CAAC,qCAAsC,GAC9C,AAAI,MAAM,CAAC,qCAAqC,EAAE,EAAM,OAAO,CAAA,CAAE,CACzE,CACF,CAGO,eAAe,EAAe,CAAQ,CAAE,EAAU,CAAC,CAAC,EACzD,GAAI,CAAC,QAAQ,GAAG,CAAC,cAAc,CAC7B,CAD+B,KACzB,AAAI,MAAM,kFAGlB,GAAI,CAWF,MAAO,CAVU,MAAM,EAAO,IAAI,CAAC,WAAW,CAAC,MAAM,CAAC,CACpD,MAAO,EAAQ,KAAK,EAAI,cACxB,SAAU,EACV,YAAa,EAAQ,WAAW,EAAI,GACpC,WAAY,EAAQ,UAAU,EAAI,IAClC,MAAO,EAAQ,KAAK,EAAI,EACxB,kBAAmB,EAAQ,iBAAiB,EAAI,EAChD,iBAAkB,EAAQ,gBAAgB,EAAI,CAChD,EAAA,EAEgB,OAAO,CAAC,EAAE,CAAC,OAAO,CAAC,OACrC,AAD4C,CAC1C,MAAO,EAAO,CAId,GAHA,QAAQ,KAAK,CAAC,4BAA6B,GAGvC,AAAiB,KAAK,GAAhB,MAAM,CACd,MAAM,AAAI,MAAM,6DACX,GAAqB,KAAK,CAAtB,EAAM,MAAM,CACrB,MAAM,AAAI,MAAM,0DACX,GAAqB,KAAK,CAAtB,EAAM,MAAM,CACrB,MAAM,AAAI,MAAM,+CAGlB,OAAM,AAAI,MAAM,CAAC,uCAAuC,EAAE,EAAM,OAAO,CAAA,CAAE,CAC3E,CACF,CAGO,eAAe,EAAgB,CAAI,CAAE,EAAY,GAAG,EACzD,IAAM,EAAW,CACf,CACE,KAAM,SACN,QAAS,6DACX,EACA,CACE,KAAM,OACN,QAAS,CAAC,gCAAgC,EAAE,EAAU;AAAA;AAAmB,EAAE,EAAA,CAAM,AACnF,EACD,CAED,OAAO,MAAM,EAAe,EAAU,CAAE,YAAa,GAAK,WAAY,GAAI,EAC5E,CAGO,eAAe,EAAa,CAAI,CAAE,EAAQ,CAAC,EAChD,IAAM,EAAW,CACf,CACE,KAAM,SACN,QAAS,4HACX,EACA,CACE,KAAM,OACN,QAAS,CAAC,SAAS,EAAE,EAAM;AAAA;AAAqC,EAAE,EAAK,SAAS,CAAC,EAAG,KAAA,CAAO,AAC7F,EACD,CAID,MAAO,CAFU,MAAM,EAAe,EAAU,CAAE,YAAa,GAAK,WAAY,GAAI,EAAA,EAGjF,KAAK,CAAC,KACN,GAAG,CAAC,GAAO,EAAI,IAAI,GAAG,WAAW,IACjC,MAAM,CAAC,GAAO,EAAI,MAAM,CAAG,GAC3B,KAAK,CAAC,EAAG,EACd,CAGO,eAAe,EAAiB,CAAI,EACzC,IAAM,EAAa,CACjB,WAAY,UAAW,SAAU,eACjC,cAAe,QAAS,OAAQ,QAAS,UAAW,QACrD,CAEK,EAAW,CACf,CACE,KAAM,SACN,QAAS,CAAC,2FAA2F,EAAE,EAAW,IAAI,CAAC,MAAM,8CAA8C,CAAC,AAC9K,EACA,CACE,KAAM,OACN,QAAS,CAAC;AAAA;AAA2B,EAAE,EAAK,SAAS,CAAC,EAAG,KAAA,CAC3D,AADkE,EAEnE,CAID,MAAO,CAFU,MAAM,EAAe,EAAU,CAAE,YAAa,GAAK,WAAY,EAAG,EAAA,EAGhF,KAAK,CAAC,KACN,GAAG,CAAC,GAAO,EAAI,IAAI,GAAG,WAAW,IACjC,MAAM,CAAC,GAAO,EAAW,QAAQ,CAAC,GACvC,CAGO,eAAe,EAAuB,CAAW,CAAE,CAAa,CAAE,EAAQ,6DAA6D,EAC5I,GAAI,CAAC,QAAQ,GAAG,CAAC,cAAc,CAC7B,CAD+B,KACzB,AAAI,MAAM,kFAGlB,GAAI,CAEF,IAAM,EAAc,EAAY,QAAQ,CAAC,UA8BzC,MAAO,CA5BU,MAAM,EAAO,IAAI,CAAC,WAAW,CAAC,MAAM,CAAC,CACpD,MAAO,SACP,SAAU,CACR,CACE,KAAM,SACN,QAAS,mOACX,EACA,CACE,KAAM,OACN,QAAS,CACP,CACE,KAAM,OACN,KAAM,CACR,EACA,CACE,KAAM,YACN,UAAW,CACT,IAAK,CAAC,KAAK,EAAE,EAAc,QAAQ,EAAE,EAAA,CAAa,CAClD,OAAQ,MACV,CACF,EACD,AACH,EACD,CACD,WAAY,IACZ,YAAa,EACf,EAAA,EAEgB,OAAO,CAAC,EAAE,CAAC,OAAO,CAAC,OAAO,AAC5C,CAAE,MAAO,EAAO,CAGd,GAFA,QAAQ,KAAK,CAAC,uBAAwB,GAEjB,KAAK,CAAtB,EAAM,MAAM,CACd,MAAU,AAAJ,MAAU,6DACX,GAAI,AAAiB,KAAK,GAAhB,MAAM,CACrB,MAAM,AAAI,MAAM,0DACX,GAAqB,MAAjB,EAAM,MAAM,EAAY,EAAM,OAAO,EAAE,SAAS,SACzD,CADmE,KAC7D,AAAI,MAAM,4DAGlB,OAAU,AAAJ,MAAU,CAAC,mCAAmC,EAAE,EAAM,OAAO,CAAA,CAAE,CACvE,CACF,CAIO,eAAe,EAA6B,CAAQ,EACzD,GAAI,CAAC,QAAQ,GAAG,CAAC,cAAc,CAC7B,CAD+B,KACzB,AAAI,MAAM,kFAGlB,IAAM,EAAK,MAAA,EAAA,CAAA,CAAA,OAEP,EAAe,KACnB,GAAI,CAIF,EAAe,MAAM,EAAO,KAAK,CAAC,MAAM,CAAC,CACvC,KAAM,EAAG,gBAAgB,CAAC,GAC1B,QAAS,WACX,GAqBA,IAAM,EAAgB,CAAC,CAhBN,MAAM,EAAO,SAAS,CAAC,MAAM,CAAC,CAC7C,MAAO,SACP,MAAO,CACL,CACE,KAAM,OACN,QAAS,CACP,CAAE,KAAM,aAAc,KAAM,6HAA8H,EAC1J,CAAE,KAAM,aAAc,QAAS,EAAa,EAAE,AAAC,EAChD,AACH,EACD,CACD,kBAAmB,IACnB,YAAa,EACf,EAAA,EAGgC,WAAW,EAAI,EAAA,CAAE,CAAE,QAAQ,GAAG,IAAI,GAGlE,GAAI,CAAC,EACH,MAAM,AAAI,MAAM,CADE,gCAIpB,OAAO,CAET,CAAE,MAAO,EAAO,CAGd,GAFA,QAAQ,KAAK,CAAC,gDAAiD,GAE1C,KAAK,CAAtB,EAAM,MAAM,CACd,MAAM,AAAI,MAAM,6DACX,GAAqB,KAAK,CAAtB,EAAM,MAAM,CACrB,MAAU,AAAJ,MAAU,0DACX,GAAqB,KAAK,CAAtB,EAAM,MAAM,CACrB,MAAM,AAAI,MAAM,sGAGlB,OAAM,AAAI,MAAM,CAAC,sCAAsC,EAAE,EAAM,OAAO,CAAA,CAAE,CAC1E,QAAU,CAER,GAAI,CACE,GAAgB,EAAa,EAAE,EAAE,CAC/B,EAAO,KAAK,EAAE,OAChB,CADwB,KAClB,EAAO,KAAK,CAAC,MAAM,CAAC,EAAa,EAAE,EAChC,EAAO,KAAK,EAAE,KAAK,AAC5B,MAAM,EAAO,KAAK,CAAC,GAAG,CAAC,EAAa,EAAE,EAI5C,CAAE,MAAO,EAAU,CACjB,QAAQ,IAAI,CAAC,8CAA+C,GAAU,SAAW,EACnF,CACF,CACF,yRChRA,IAAA,EAAA,EAAA,CAAA,CAAA,OACA,EAAA,EAAA,CAAA,CAAA,OACA,EAAA,EAAA,CAAA,CAAA,yCAKA,IAAM,EAAgB,IAAI,IASnB,GAT0B,YASX,EAAwB,CAAM,CAAE,CAAQ,CAAE,CAAQ,CAAE,EAAW,IAAI,EACvF,GAAI,CACF,IAAI,EAAO,GAX2D,AAYlE,EAAW,UACb,WACA,EACA,MAAO,EACP,UAAW,CACb,EAEA,GAAiB,oBAAb,GAAkC,EAAS,WAAW,GAAG,QAAQ,CAAC,QAAS,CAE7E,GAAM,CAAE,8BAA4B,CAAE,CAAG,MAAA,EAAA,CAAA,CAAA,OAInC,EAAK,MAAA,EAAA,CAAA,CAAA,OACL,EAAO,MAAA,EAAA,CAAA,CAAA,OAGP,EAFK,AAEK,OAFL,EAAA,CAAA,CAAA,KAAA,EAEQ,MAAM,GACnB,EAAe,EAAK,IAAI,CAAC,EAAS,CAAC,KAAK,EAAE,KAAK,GAAG,GAAG,CAAC,EAAE,EAAA,CAAU,EAExE,GAAI,CAWF,GATA,EAAG,aAAa,CAAC,EAAc,GAM/B,EAAO,CAHP,AAGQ,GAHD,MAAM,EAA6B,EAAA,GAG1B,EAAA,CAAE,CAAE,QAAQ,GAAG,IAAI,GACnC,EAAS,cAAc,CAAG,qBAEtB,CAAC,GAAwB,GAAG,CAAnB,EAAK,MAAM,CACtB,MAAM,AAAI,MAAM,+CAIpB,QAAU,CAER,GAAI,CACE,EAAG,UAAU,CAAC,IAChB,EAAG,SAD4B,CAClB,CAAC,EAElB,CAAE,MAAO,EAAc,CACrB,QAAQ,IAAI,CAAC,kCAAmC,EAClD,CACF,CACF,MACK,GAAI,EAAS,QAAQ,CAAC,qBAAuB,EAAS,WAAW,GAAG,QAAQ,CAAC,SAKhF,CAL0F,CAKnF,CAHP,EAAO,CADQ,MAAM,EAAA,OAAO,CAAC,cAAc,CAAC,QAAE,CAAO,EAAA,EACvC,KAAA,AAAK,EAGP,OAAO,CAAC,OAAQ,KAAK,IAAI,QAElC,GAAI,EAAS,QAAQ,CAAC,gBAAkB,EAAS,WAAW,GAAG,KAAK,CAAC,sBAAuB,CAC/F,IAAM,EAAW,EAAA,IAAS,CAAC,EAAQ,CAAE,KAAM,QAAS,GAC9C,EAAS,EAAS,UAAU,CAG5B,EAAU,EAAE,CAClB,IAAK,IAAM,KAAa,EAAQ,CAC9B,IAAM,EAAQ,EAAS,MAAM,CAAC,EAAU,CAGxC,IAAK,IAAM,KAFO,EAEA,AAFA,KAAU,CAAC,IAEA,SAFa,CAAC,EAAO,CAAE,OAAQ,EAAG,OAAQ,EAAG,GAGxE,GAAI,MAAM,OAAO,CAAC,GAAM,CACtB,IAAM,EAAU,EAAI,MAAM,CAAC,GAAQ,GAAQ,EAAK,QAAQ,GAAG,IAAI,IAAI,IAAI,CAAC,KACpE,EAAQ,IAAI,IAAI,AAClB,EAAQ,IAAI,CAAC,CAAC,CAAC,EAAE,EAAU,EAAE,EAAE,EAAA,CAAS,CAE5C,CAEJ,CAEA,EAAO,EAAQ,IAAI,CAAC,KACtB,MACK,GAAI,EAAS,UAAU,CAAC,UAAY,EAAS,WAAW,GAAG,QAAQ,CAAC,QACvE,CADgF,CACzE,EAAO,QAAQ,CAAC,cAEpB,GAAI,EAAS,UAAU,CAAC,WAAa,EAAS,WAAW,GAAG,KAAK,CAAC,sDAErE,CAF4H,EAExH,CACF,GAAM,wBAAE,CAAsB,CAAE,CAAG,MAAA,EAAA,CAAA,CAAA,OAGnC,GAAI,CAQF,EANmB,KAMZ,CANkB,EACvB,EACA,EACA,4MAOF,EAAS,cAAc,CAAG,eAC5B,CAAE,MAAO,EAAa,CAIpB,IAAM,EAAY,CAAC,MAAA,EAAA,CAAA,CAAA,MAAA,CAA4B,CAAE,OAAO,CAClD,EAAc,GAAY,EAEhC,GAAI,CASF,EAAO,CARQ,MAAM,EAAU,SAAS,CAAC,EAAa,UAAW,CAC/D,OAAQ,AAAC,IACH,EAAE,MAAM,AAGd,CACF,EAAA,EAJqB,AAMP,IAAI,CAAC,IAAI,CAAC,IAAI,GAC5B,EAAS,CAPgC,aAOlB,CAAG,eAC5B,CAAE,MAAO,EAAU,CAIjB,IAAM,EAAS,MAAM,EAAU,YAAY,CAAC,UAAW,EAAG,CACxD,OAAQ,AAAC,IACH,EAAE,MAAM,AAGd,CACF,GAEA,CANqB,EAMjB,CAEF,EAAO,CADQ,MAAM,EAAO,MAPW,GAOF,CAAC,EAAA,EACxB,IAAI,CAAC,IAAI,CAAC,IAAI,GAC5B,EAAS,cAAc,CAAG,sBAC5B,QAAU,CACR,MAAM,EAAO,SAAS,EACxB,CACF,CACF,CAEA,GAAI,CAAC,GAAwB,GAAG,CAAnB,EAAK,MAAM,CACtB,MAAU,AAAJ,MAAU,6GAIpB,CAAE,MAAO,EAAY,CAEnB,MADA,QAAQ,KAAK,CAAC,oCAAqC,GAC7C,AAAI,MAAM,CAAC,mCAAmC,EAAE,EAAW,OAAO,CAAA,CAAE,CAC5E,MAGA,MAAM,AAAI,MAAM,CAAC,6BAA6B,EAAE,EAAA,CAAU,EAG5D,GAAI,CAAC,GAAQ,AAAuB,GAAG,GAArB,IAAI,GAAG,MAAM,CAC7B,MAAM,AAAI,MAAM,uCAKlB,OAFA,EAAS,SAAS,CAAG,EAAK,KAAK,CAAC,OAAO,MAAM,CAEtC,MAAE,WAAM,CAAS,CAC1B,CAAE,MAAO,EAAO,CAEd,MADA,QAAQ,KAAK,CAAC,2BAA4B,GACpC,AAAI,MAAM,CAAC,kCAAkC,EAAE,EAAM,OAAO,CAAA,CAAE,CACtE,CACF,CAMO,eAAe,EAAiB,CAAI,CAAE,EAAY,GAAI,CAAE,EAAU,GAAG,EAC1E,IAAM,EAAY,EAAK,KAAK,CAAC,aAAa,MAAM,CAAC,GAAK,EAAE,IAAI,GAAG,MAAM,CAAG,IAClE,EAAS,EAAE,CAEb,EAAe,GACf,EAAgB,EAEpB,IAAK,IAAM,KAAY,EAAW,CAChC,IAAM,EAAiB,EAAS,MAAM,CAElC,EAAgB,EAAiB,GAAa,EAAa,IAAI,IAAI,AAErE,EAAO,IAAI,CAAC,CACV,KAAM,EAAa,IAAI,GACvB,WAAY,EAAK,OAAO,CAAC,EAAa,IAAI,IAC1C,SAAU,EAAK,OAAO,CAAC,EAAa,IAAI,IAAM,EAAa,MAAM,AACnE,GAMA,EAAgB,CADhB,EAFc,AACO,AACN,EAFY,KAAK,CAAC,OACN,KAAK,CAAC,CAAC,KAAK,KAAK,CAAC,EAAU,KAC3B,IAAI,CAAC,KAAO,IAAM,EAAW,GAAA,EAC5B,MAAM,GAEnC,GAAgB,EAAW,KAC3B,GAAiB,EAAiB,EAEtC,CAGI,EAAa,IAAI,IAAI,AACvB,EAAO,IAAI,CAAC,CACV,KAAM,EAAa,IAAI,GACvB,WAAY,EAAK,OAAO,CAAC,EAAa,IAAI,IAC1C,SAAU,EAAK,MAAM,AACvB,GAGF,IAAM,EAAc,EAAO,MAAM,CAAG,EAAI,EAAS,CAAC,CAAE,KAAM,EAAK,IAAI,GAAI,WAAY,EAAG,SAAU,EAAK,MAAO,AAAD,EAAG,CAGxG,CAAE,yBAAuB,CAAE,CAAG,MAAA,EAAA,CAAA,CAAA,OAEpC,GAAI,CAGF,IAAK,IAAI,EAAI,EAAG,EAAI,EAAY,MAAM,CAAE,KAAK,CAAY,CACvD,IAAM,EAAQ,EAAY,KAAK,CAAC,EAAG,EAFlB,EAEsB,EAEvC,CADmB,MAAM,EAAwB,EAAM,GAAG,CAAC,GAAK,EAAE,IAAI,EAAA,EAC3D,OAAO,CAAC,CAAC,EAAK,KACvB,CAAK,CAAC,EAAI,CAAC,SAAS,CAAG,CACzB,EACF,CACF,CAAE,MAAO,EAAU,CACjB,QAAQ,IAAI,CAAC,sDAAuD,EAAS,OAAO,EACpF,GAAM,mBAAE,CAAiB,CAAE,CAAG,MAAA,EAAA,CAAA,CAAA,OAC9B,IAAK,IAAM,KAAS,EAClB,GAAI,CACF,EAAM,IAFuB,KAEd,CAAG,MAAM,EAAkB,EAAM,IAAI,CACtD,CAAE,MAAO,EAAK,CACZ,QAAQ,IAAI,CAAC,0CAA2C,EAAI,OAAO,EACnE,EAAM,SAAS,CAAG,IACpB,CAEJ,CAEA,OAAO,CACT,CAgDO,eAAe,EAAe,CAAY,CAAE,CAAM,CAAE,CAAK,CAAE,EAAO,CAAC,EACxE,GAAI,CAAC,GAA4B,GAAG,CAArB,EAAO,MAAM,CAC1B,MAAO,EAAE,CAIX,GAAM,mBAAE,CAAiB,CAAE,CAAG,MAAA,EAAA,CAAA,CAAA,OAC1B,EAAiB,KAErB,GAAI,CACF,EAAiB,MAAM,EAAkB,EAC3C,CAAE,MAAO,EAAU,CACjB,QAAQ,IAAI,CAAC,kDAAmD,EAClE,CA4CA,OAzCqB,AAyCd,AAPc,EAlCO,GAAG,CAAC,CAAC,EAAO,KACtC,IAAI,EAAgB,EAIlB,EADE,GAAkB,EAAM,SAAS,CACnB,AAhEtB,CA+D2C,QA/DlC,AAAiB,CAAI,CAAE,CAAI,EAClC,GAAI,CAAC,GAAQ,CAAC,GAAQ,EAAK,MAAM,GAAK,EAAK,MAAM,CAAE,OAAO,EAE1D,IAAM,EAAM,EAAK,MAAM,CAAC,CAAC,EAAK,EAAK,IAAM,EAAM,EAAM,CAAI,CAAC,EAAE,CAAE,GACxD,EAAO,KAAK,IAAI,CAAC,EAAK,MAAM,CAAC,CAAC,EAAK,IAAQ,EAAM,EAAM,EAAK,IAC5D,EAAO,KAAK,IAAI,CAAC,EAAK,MAAM,CAAC,CAAC,EAAK,IAAQ,EAAM,EAAM,EAAK,IAElE,OAAO,GAAO,EAAO,CAAR,AAAQ,CAAI,AAC3B,EAwDuC,EAAgB,EAAM,SAAS,EAnDtE,AAsDsB,SAtDE,AAAf,CAAmB,CAAE,CAAK,EACjC,IAAM,EAAY,EAAK,WAAW,GAAG,KAAK,CAAC,OAAO,MAAM,CAAC,GAAK,EAAE,MAAM,CAAG,GACnE,EAAa,EAAM,WAAW,GAAG,KAAK,CAAC,OAAO,MAAM,CAAC,GAAK,EAAE,MAAM,CAAG,GAE3E,GAA0B,IAAtB,EAAW,MAAM,CAAQ,OAAO,EAEpC,IAAI,EAAQ,EACN,EAAgB,CAAC,EAgBvB,OAfA,EAAU,OAAO,CAAC,IAChB,CAAa,CAAC,EAAK,CAAG,AAAC,EAAa,CAAC,EAAK,GAAI,CAAC,CAAI,CACrD,GAEA,EAAW,OAAO,CAAC,IACjB,IAAM,EAAY,CAAa,CAAC,EAAU,EAAI,EAC9C,GAAI,EAAY,EAAG,CAEjB,IAAM,EAAK,EAAY,EAAU,MAAM,CAEjC,EAAM,KAAK,GAAG,CAAC,EAAU,MAAM,EAAI,CAAD,EAAa,CAAC,EAAK,EAC3D,GAAS,EAAK,CAChB,CACF,GAEO,EAAQ,EAAW,MAAM,AAClC,EA8BqC,EAAM,IAAI,CAAE,GAAS,GAItD,CAJ0D,GAIpD,EAAa,EAAM,IAAI,CAAC,WAAW,GACnC,EAAa,EAAM,UALyE,CAK9D,GAChC,EAAkB,EAClB,EAAW,QAAQ,CAAC,KACtB,EAAkB,EAAA,EAIpB,EALqC,EAK/B,EAAa,EAAM,WAAW,GAAG,KAAK,CAAC,OAAO,MAAM,CAAC,GAAK,EAAE,MAAM,CAAG,GAErE,EADgB,AACA,EADW,MAAM,CAAC,GAAQ,EAAW,QAAQ,CAAC,IAAO,MAAM,CAC3C,EAAW,MAAM,CAAI,IAErD,EAAa,EAAgB,EAAkB,EAErD,MAAO,CACL,GAAG,CAAK,CACR,MAAO,QACP,CACF,CACF,GAIG,IAAI,CAAC,CAAC,EAAG,IAAM,EAAE,KAAK,CAAG,EAAE,KAAK,EAChC,KAAK,CAAC,EAAG,GAKQ,MAAM,CAAC,GAAS,EAAM,KAAK,EAAI,EACrD,CASO,eAAe,EAAe,CAAK,CAAE,CAAc,CAAE,CAAY,CAAE,EAAsB,EAAE,CAAE,EAAmB,CAAC,CAAC,EACvH,GAAI,CAAC,GAA4C,GAAG,CAA7B,EAAe,MAAM,CAC1C,MAAO,CACL,OAAQ,0KACR,WAAY,EACZ,QAAS,EAAE,AACb,EAIF,IAAM,EAAkB,EACrB,GAAG,CAAC,CAAC,EAAO,IAAQ,CAAC,SAAS,EAAE,EAAM,EAAE;AAAG,EAAE,EAAM,IAAI,CAAA,CAAE,EACzD,IAAI,CAAC,eAGF,EAAa,KAAK,GAAG,CAA2B,GAA1B,CAAc,CAAC,EAAE,CAAC,KAAK,CAAO,GAE1D,GAAI,CAEF,IAAI,EAAiB,GACjB,GAAuB,EAAoB,IAAI,IAAI,CACrD,EAAiB,CAAC;AAAA;AAAA;AACxB,EAAE,oBAAoB;;uGAEgF,AAAC,EAGnG,IAAM,EAAW,CACf,CACE,KAAM,SACN,QAAS,CAAC;;;;;;;;;;;;;;;;;;;mDAmBiC,CAC7C,AAD8C,EAE9C,CACE,KAAM,OACN,QAAS,CAAC;;;AAGlB,EAAE,EAAA,EAAkB,eAAe;;;AAGnC,EAAE,MAAM;;;;;;;yFAO8E,CAAC,AACjF,EACD,CAEG,EAAS,MAAM,CAAA,EAAA,EAAA,cAAc,AAAd,EAAe,EAAU,CAC1C,MAAO,cACP,YAAa,GACb,WAAY,KACZ,MAAO,GACP,kBAAmB,GACnB,iBAAkB,EACpB,EADwB,CAUxB,OARA,EAAS,EAAO,IAAI,GAEL,CAJ6B,wBAIJ,IAAI,CAAC,IAAW,EAAO,MAAM,CAAG,KAItE,GAFiB,CAAC,KAET;;AADA,EAAE,EAAe,KAAK,CAAC,EAAE,GAAG,GAAG,CAAC,GAAG,KAAK,EAAE,IAAI,CAAC,SAAS,CAAC,EAAE,KAAK,OAAO,CAAC,OAAO,MAAO,CAAD,CAAG,IAAI,CAAC,MAAM,CAAC,IAAI,MAAM,EAAA,CAAG,EAAG,IAAI,CAAC,MAAM;AAAA;AAAc,EAAE,EAAe,GAAG,CAAC,CAAC,EAAE,IAAI,CAAC,UAAU,EAAE,EAAE,EAAE,QAAQ,EAAE,EAAE,KAAK,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,EAAE,IAAI,CAAC,MAAM;AAAA;AAAA,uFAAsK,CAAC,AAC7Y,EAEJ,CACL,oBACA,EACA,QAAS,EAAe,GAAG,CAAC,IAAK,AAAC,CAChC,SAAU,GAAkB,kBAAoB,GAAkB,UAAY,YAC9E,KAAM,EAAE,IAAI,CAAC,SAAS,CAAC,EAAG,KAAO,MACjC,MAAO,EAAE,KAAK,CAAC,OAAO,CAAC,GACzB,CAAC,CACH,CACF,CAAE,MAAO,EAAO,CACd,QAAQ,KAAK,CAAC,sDAAuD,GAErE,IAAM,EAAc,CAAc,CAAC,EAAE,CAAC,IAAI,CAC1C,MAAO,CACL,OAAQ,CAAC;AAAA;AAA4B,EAAE,YAAY;AAAA;AAAI,EAAE,EAAe,MAAM,CAAG,EAAI,+BAAiC,EAAe,KAAK,CAAC,EAAG,GAAG,GAAG,CAAC,CAAC,EAAG,IAAM,CAAC,EAAE,EAAE,EAAE,IAAI,CAAC,SAAS,CAAC,EAAG,KAAK,GAAG,CAAC,EAAE,IAAI,CAAC,MAAQ,GAAA,CAAI,CACpN,aACA,QAAS,EAAe,GAAG,CAAC,IAAK,AAAC,CAChC,SAAU,GAAkB,kBAAoB,GAAkB,UAAY,YAC9E,KAAM,EAAE,IAAI,CAAC,SAAS,CAAC,EAAG,KAAO,MACjC,MAAO,EAAE,KAAK,CAAC,OAAO,CAAC,GACzB,CAAC,CACH,CACF,CACF,CAKO,eAAe,EAAc,CAAM,CAAE,CAAI,CAAE,EAAW,CAAC,CAAC,EAG7D,IAAM,EAAS,MAAM,EAAiB,GAIhC,EAAe,CACnB,OACA,SACA,SAAU,CACR,GAAG,CAAQ,CACX,SAAU,EAAS,QAAQ,EAAI,EAAS,gBAAgB,EAAI,UAC5D,SAAU,IAAI,OAAO,WAAW,GAChC,WAAY,EAAO,MAAM,AAC3B,CACF,EAOA,OALA,EAAc,GAAG,CAAC,EAAQ,GAKnB,QACL,EACA,WAAY,EAAO,MAAM,CACzB,UAAW,EAAK,KAAK,CAAC,OAAO,MAAM,AACrC,CACF,CAmBO,eAAe,EAAmB,CAAK,CAAE,EAAU,IAAI,EAK5D,IAAM,EAAY,GAAW,EAAQ,MAAM,CAAG,EAC1C,EAAQ,GAAG,CAAC,IACV,IAAM,EAAO,EAAc,GAAG,CAAC,GAE/B,MAAO,CAAE,UAAI,CAAK,CACpB,GAAG,MAAM,CAAC,GAAK,EAAE,IAAI,EACrB,MAAM,IAAI,CAAC,EAAc,OAAO,IAAI,GAAG,CAAC,CAAC,CAAC,EAAI,EAAK,IAE1C,IAAE,OAAI,CAAK,IAKxB,GAAyB,GAAG,CAAxB,EAAU,MAAM,CAElB,MAAO,EAAE,CAGX,IAAM,EAAa,EAAE,CAErB,IAAK,GAAM,IAAE,CAAE,MAAE,CAAI,CAAE,GAAI,EAAW,CACpC,GAAI,CAAC,GAAQ,CAAC,EAAK,MAAM,EAAI,CAAC,EAAK,MAAM,CAAC,MAAM,CAE9C,CAFgD,QAMlD,IAAM,EAAU,MAAM,EAAe,EAAK,IAAI,CAAE,EAAK,MAAM,CAAE,EAAO,GAEhE,EAAQ,MAAM,CAAG,GAAG,AAEtB,EAAW,IAAI,CAAC,CACd,OAAQ,EACR,SAAU,EAAK,QAAQ,EAAE,kBAAoB,EAAK,QAAQ,EAAE,UAAY,kBACxE,EACA,SAAU,CAAO,CAAC,EAAE,CAAC,KAAK,AAC5B,EAIJ,CAKA,OAAO,EAAW,IAAI,CAAC,CAAC,EAAG,IAAM,EAAE,QAAQ,CAAG,EAAE,QAAQ,CAC1D,CAQO,eAAe,EAA4B,CAAK,CAAE,CAAa,CAAE,EAAsB,EAAE,QAC9F,GAAI,CAAC,GAAiB,AAAyB,GAAG,GAAd,MAAM,CACxC,MAAO,CACL,OAAQ,gJACR,WAAY,EACZ,QAAS,EAAE,AACb,EAIF,IAAM,EAAY,EACf,OAAO,CAAC,GACP,EAAW,OAAO,CAAC,GAAG,CAAC,IAAK,AAAC,CAC3B,GAAG,CAAC,CACJ,SAAU,EAAW,QAAQ,CAC7B,OAAQ,EAAW,MAAM,CAC3B,CAAC,GAEF,IAAI,CAAC,CAAC,EAAG,IAAM,EAAE,KAAK,CAAG,EAAE,KAAK,EAChC,KAAK,CAAC,EAAG,GAEZ,GAAyB,GAAG,CAAxB,EAAU,MAAM,CAClB,MAAO,CACL,OAAQ,0DACR,WAAY,EACZ,QAAS,EAAE,AACb,EAIF,IAAM,EAAe,CAAS,CAAC,EAAE,CAC3B,GAxGoB,EAwGG,EAAa,EAxGV,EAwGf,EAA+B,CAvGzC,EAAc,GAAG,CAAC,IAyGzB,GAA6B,GAAG,CAA5B,EAAc,MAAM,CAEtB,OAAO,MAAM,EAAe,EAAO,EAAW,EAAS,IAAI,CAAE,EAAqB,EAAS,QAAQ,EAAI,CAAC,EACnG,EAEL,IAAM,EAAa,KAAK,GAAG,CAAC,AAAqB,KAAR,KAAK,CAAO,GAErD,GAAI,CAEF,IAAM,EAAkB,EACrB,GAAG,CAAC,CAAC,EAAO,IAAQ,CAAC,aAAa,EAAE,EAAM,QAAQ,CAAC,YAAY,EAAE,EAAM,EAAE;AAAG,EAAE,EAAM,IAAI,CAAA,CAAE,EAC1F,IAAI,CAAC,eAGJ,EAAiB,GACjB,GAAuB,EAAoB,IAAI,IAAI,CACrD,EAAiB,CAAC;AAAA;AAAA;AAC1B,EAAE,oBAAoB;;sGAEgF,CAAC,EAGjG,IAAM,EAAW,CACf,CACE,KAAM,SACN,QAAS,CAAC;;;;;;;;;;;;;oCAagB,CAAC,AAC7B,EACA,CACE,KAAM,OACN,QAAS,CAAC,kCAAkC,EAAE,EAAc,MAAM,CAAC;;;AAG7E,EAAE,EAAA,EAAkB,eAAe;;;AAGnC,EAAE,MAAM;;;;;;;yFAO8E,CAAC,AAC/E,EACD,CAEG,EAAS,MAAM,CAAA,EAAA,EAAA,cAAA,AAAc,EAAC,EAAU,CAC1C,MAAO,cACP,YAAa,GACb,WAAY,KACZ,MAAO,GACP,kBAAmB,GACnB,iBAAkB,EACpB,GAGA,GAFA,CAEI,CAFK,EAAO,IAAI,GACL,yBAAyB,IAAI,CAAC,IAAW,EAAO,MAAM,CAAG,IAC5D,CACV,IAAM,EAAa,EAAU,KAAK,CAAC,EAAE,GAAG,GAAG,CAAC,CAAC,EAAE,IAAI,CAAC,EAAE,EAAE,EAAE,QAAQ,CAAC,QAAQ,EAAE,EAAE,KAAK,CAAC,OAAO,CAAC,GAAG,GAAG,EAAE,EAAE,IAAI,CAAC,SAAS,CAAC,EAAE,KAAK,OAAO,CAAC,OAAO,KAAA,EAAO,EAAE,IAAI,CAAC,MAAM,CAAC,IAAI,MAAM,GAAA,CAAI,EAAE,IAAI,CAAC,MACtL,EAAS,CAAC;;;AAEN,EAAE,WAAW;;2GAE4E,CAAC,AAChG,CACA,MAAO,QACL,aACA,EACA,QAAS,EAAU,KAAK,CAAC,EAAG,GAAG,GAAG,CAAC,GAAM,CAAD,CACtC,SAAU,EAAE,QAAQ,CACpB,KAAM,EAAE,IAAI,CAAC,SAAS,CAAC,EAAG,KAAO,MACjC,MAAO,EAAE,KAAK,CAAC,OAAO,CAAC,GACzB,CAAC,CACH,CACF,CAAE,MAAO,EAAO,CACd,QAAQ,KAAK,CAAC,sEAAuE,GAErF,IAAI,EAAS,CAAC,cAAc,EAAE,EAAc,MAAM,CAAC;AAAA;AAAwB,CAAC,CAG5E,GAFA,GAAU,CAAC,iBAAiB,EAAE,EAAa,QAAQ,CAAC;AAAM,EAAE,EAAa,IAAI,CAAC;AAAA;AAAI,CAAC,CAE/E,EAAU,MAAM,CAAG,EAAG,CACxB,IAAM,EAAY,EAAU,KAAK,CAAC,EAAG,GAAG,MAAM,CAAC,GAAK,EAAE,MAAM,GAAK,EAAa,MAAM,EAChF,EAAU,MAAM,CAAG,GAAG,CACxB,GAAU,CAAC;AAAgD,CAAC,CAC5D,EAAU,OAAO,CAAC,CAAC,EAAO,KACxB,GAAU,CAAC;AAAA,CAAG,EAAE,EAAM,EAAE,MAAM,EAAE,EAAM,QAAQ,CAAC;AAAI,EAAE,EAAM,IAAI,CAAC,SAAS,CAAC,EAAG,KAAK;AAAK,CAAC,AAC1F,GAEJ,CAEA,IAAM,EAAa,KAAK,GAAG,CAAsB,GAArB,EAAa,KAAK,CAAO,GAErD,MAAO,CACL,OAAQ,EAAO,IAAI,cACnB,EACA,QAAS,EAAU,KAAK,CAAC,EAAG,GAAG,GAAG,CAAC,IAAK,AAAC,CACvC,SAAU,EAAE,QAAQ,CACpB,KAAM,EAAE,IAAI,CAAC,SAAS,CAAC,EAAG,KAAO,MACjC,MAAO,EAAE,KAAK,CAAC,OAAO,CAAC,GACzB,CAAC,CACH,CACF,CACF,CACF,CAeO,SAAS,IACd,IAAM,EAAY,MAAM,IAAI,CAAC,EAAc,MAAM,IAEjD,MAAO,CACL,eAAgB,EAAc,IAAI,CAClC,WAAY,EAAU,MAAM,CAAC,CAAC,EAAK,IAAQ,GAAO,EAAI,CAAL,GAAS,CAAC,KAAK,CAAC,OAAO,MAAM,GAAI,CAAC,CAAG,GACtF,YAAa,EAAU,MAAM,CAAC,CAAC,EAAK,IAAQ,GAAO,EAAI,CAAL,KAAW,EAAE,SAAU,CAAC,CAAG,GAC7E,UAAW,MAAM,IAAI,CAAC,EAAc,OAAO,IAAI,GAAG,CAAC,CAAC,CAAC,EAAQ,EAAI,GAAK,CAAC,QACrE,EACA,MAAO,MAAM,IAAI,CAAC,EAAc,IAAI,IAAI,OAAO,CAAC,GAChD,SAAU,EAAI,QAAQ,EAAE,UAAY,EAAI,QAAQ,EAAE,kBAAoB,UACtE,UAAW,EAAI,IAAI,CAAC,KAAK,CAAC,OAAO,MAAM,CACvC,WAAY,EAAI,MAAM,EAAE,QAAU,EAClC,SAAU,EAAI,QAAQ,EAAE,SAC1B,CAAC,CACH,CACF"}